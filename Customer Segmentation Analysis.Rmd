---
title: "Customer Segmentation"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem Definition

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.

## Data Sourcing

Data being used in this study was provided by Moringa School

**Data Description**

* The dataset consists of 10 numerical and 8 categorical attributes.

* The **Revenue** attribute can be used as the **class label**.

* "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another. 

**Metrics Measured by Google Analytics**

1. The value of the **Bounce Rate** feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. 
2. The value of the **Exit Rate** feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.
3. The **Page Value** feature represents the average value for a web page that a user visited before completing an e-commerce transaction.


* The **Special Day** feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.

* The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.

## Reading/Checking data

```{r}
#Loading the data
df <- online_shoppers_intention
#Showing head of data
head(df)
```
```{r}
#Checking for dimensions and class types
str(df)


```
Data has 12330 observations(rows) and 18 variables(columns)
We have integers, numerics , character and logical features in our dataset.
```{r}
summary((df))
```
## Data Cleaning

```{r}
# Changing the type of the loaded dataset to a dataframe
df = as.data.frame(df)

# Cleaning column names, by making them uniform
colnames(df) = tolower(colnames(df))

```

```{r}
#Checking for duplicated rows
duplicated_rows <- df[duplicated(df),]
dim(duplicated_rows)

#Dropping duplicated rows
df <- df[!duplicated(df), ]
dim(df)
```

```{r}
#Checking for missing values
colSums(is.na(df))

```
Since this data is generated from a website, it doesn't make sense that we have missing values. We will therefore drop this values.
```{r}
#Dropping  missing values
df = na.omit(df)
colSums(is.na(df))
dim(df)

```
```{r}
library(tidyverse)
cat_cols = c('month', 'operatingsystems',	'browser',	'region',	'traffictype', 'visitortype')
library(magrittr)
# Changing columns to factors
df[,cat_cols] %<>% lapply(function(x) as.factor(as.character(x)))
str(df)                        
```
```{r}
head(df)
```


```{r}
colnames(df)
#Checking for outliers
#First we select numeric columns 
nums <- subset(df, select = -c(specialday, month, operatingsystems,browser, region, traffictype, visitortype,weekend,revenue))
head(nums)

boxplot(nums)
```
All of the numerical columns have outliers. It is also important to note that a few of this have negative values. But since we are dealing with customers and retails have all sort of customers who have different values and capabilities we will leave these outliers as they are. This way we will be able to capture this groups when grouping the customers.
We take the negative to mean that a visitor moved back to a page they were on previously. 

## Data Exploration

### 1. Distributions of numerical values

```{r}
library(ggplot2)
library(psych)
```

```{r}
#Central tendency 
describe(nums)

```

```{r}
#Plotting histograms to show distribution of variables 
par(mfrow = c(2, 2))
hist(nums$administrative)
hist(nums$informational)
hist(nums$bouncerates)
hist(nums$exitrates)
```

```{r}
par(mfrow = c(2, 2))
hist(nums$administrative_duration)
hist(nums$informational_duration)
hist(nums$productrelated_duration)
hist(nums$pagevalues)
```

**Conclusions**
From the central tendency we see that:
1. All variables have a sample size of 12199
2. Product related duration have the largest figures and range, meaning peole visiting the website spend alot of time in the product related page
3. People also spend a considerable amount of time checking on the administration
4. People spend the least of time checking out the information related page

From the above distributions we can conclude that
1. Our numerical values are skewed to the left
2. They don't follow a normal distribution
3. Variables dealing with duration have larger values because they represent duration of user on page
4. Exit rates vary alot 

### 2. Categorical data Analysis

```{r}
install.packages("ggpubr")
library(ggpubr)
```

```{r}
#Did most traffic generate any revenue?
r <- ggplot(data = df) +
  geom_bar(mapping = aes(x = revenue))
#Was traffic high on weekends or not?
w <- ggplot(data = df) +
  geom_bar(mapping = aes(x = weekend))
#What group of visitors frequented the website?
v <-ggplot(data = df) +
  geom_bar(mapping = aes(x = visitortype))
#What traffic type was mostly used?
t <- ggplot(data = df) +
  geom_bar(mapping = aes(x = traffictype))
ggarrange(r, w, v, t + rremove("x.text"), 
          ncol = 2, nrow = 2)
```

```{r}
#Which months had the highest traffic
m <- ggplot(data = df) +
  geom_bar(mapping = aes(x = month))
#Distribution of operating systems on traffic
o <- ggplot(data = df) +
  geom_bar(mapping = aes(x = operatingsystems))
#Browser distribution
b <-ggplot(data = df) +
  geom_bar(mapping = aes(x = browser))
#Which regions trafficked the website the most?
r <- ggplot(data = df) +
  geom_bar(mapping = aes(x = region))
ggarrange(m, o, b, r + rremove("x.text"), 
          ncol = 2, nrow = 2)
```

**Conclusions**
1. Most of the traffic in the website doesn't generate any revenue
2. There is more traffic on weekdays than weekends, but the traffic on weekends is relatively high considering that weekends consist of only 2 days per week.
3. Most of the people visiting the website are returning visitors, only a small percentage are new 
4. There is alot of traffic in the website in May, November, March and Dec
5. Almost 5000 of the traffic in the website for the year was from region 1, around 2,300 from region 3 and the other regions ranging from 1000 to 300 individuals.

### 3. Bivariate Analysis

```{r}
#Revenue generation per month
df %>% 
  ggplot() +
  aes(x = month, revenue = ..count../nrow(df), fill = revenue) +
  geom_bar() +
  ylab("relative frequency")

```



```{r}
#Checking how weekends generated revenue as compared to weekdays
ggplot(df, 
       aes(x = revenue, 
           fill = weekend)) + 
  geom_bar(position = "stack")

```

```{r}
df %>% 
  ggplot() +
  aes(x = region, revenue = ..count../nrow(df), fill = revenue) +
  geom_bar() +
  ylab("relative frequency")

```

```{r}
df %>% 
  ggplot() +
  aes(x = traffictype, revenue = ..count../nrow(df), fill = revenue) +
  geom_bar() +
  ylab("relative frequency")

```

**Conclusions**
1. Most of the revenue is generated during weekdays, but a relative amount is generated on weekends
2. The month of November had most revenue, i.e most people who visit the site actually purchased
3. Traffic from region 1 seem to purchase from the website but also they are the most frequent
4. Traffic type 2 has the highest revenue count


```{r}
#Checking the distribution of different variables in relation to revenue
options(repr.plot.width = 11, repr.plot.height = 5)
p1 = ggplot(df, aes(productrelated, col = revenue)) + 
  geom_density(aes(fill = revenue), alpha = 0.4) + 
  labs(x = 'Product related', y = 'Density', title = '') + 
  theme(legend.position = 'none', 
       plot.title = element_text(size = 12)) 

p2 = ggplot(df, aes(bouncerates, col = revenue)) + 
  geom_density(aes(fill = revenue), alpha = 0.4) + 
  labs(x = 'Bouncerates', y = '', title = '') + 
  theme(legend.position = 'top') 

p3 = ggplot(df, aes(exitrates, col = revenue)) + 
  geom_density(aes(fill = revenue), alpha = 0.4) + 
  labs(x = 'exitrates', y = '', title = '') + 
  theme(legend.position = 'none', 
       plot.title = element_text(size = 12)) 


p4 = ggplot(df, aes(informational, col = revenue)) + 
  geom_density(aes(fill = revenue), alpha = 0.4) + 
  labs(x = 'informational', y = '', title = '') + 
  theme(legend.position = 'none', 
       plot.title = element_text(size = 12)) 


ggarrange(p1, p2, p3, p4 + rremove("x.text"), 
          ncol = 2, nrow = 2)
```




## Implement the Solution

### Feature Engineering

```{r}
#converting the variable weekend to a dummy
#with weekend being a ‘1’ and a weekday being a ‘0’
mod.data <- df %>%
  mutate(Weekend_binary = ifelse(weekend == "FALSE",0,1))
head(mod.data)

```
```{r}
#Removing the target column and weekday column
mod <- subset(mod.data, select = -c(weekend))
#Separating features from target
mod.new <- mod[, c(1,2,3,4,5,6,7,8,9,10)]
mod.class <- mod[, "revenue"]
head(mod.new)
```


```{r}
#Normalizing data

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
## Creating a copy of the original data.
data.mod <- mod.new

## Normalizing our 10 variables.
data.mod$administrative <- normalize(data.mod$administrative)
data.mod$administrative_duration <- normalize(data.mod$administrative_duration)
data.mod$informational <- normalize(data.mod$informational)
data.mod$informational_duration <- normalize(data.mod$informational_duration)
data.mod$productrelated <- normalize(data.mod$productrelated)
data.mod$productrelated_duration <- normalize(data.mod$productrelated_duration)
data.mod$bouncerates <- normalize(data.mod$bouncerates)
data.mod$exitrates <- normalize(data.mod$exitrates)
data.mod$pagevalues <- normalize(data.mod$pagevalues)
data.mod$specialday <- normalize(data.mod$specialday)
head(data.mod)
```

### Modelling
**1. K-Means Clustering**

```{r}
#Load the packages as follow:


library(factoextra)
library(NbClust)
```

```{r}
#Getting the optimal number of clusters 
# Silhouette method
fviz_nbclust(data.mod, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")

```
```{r}
#Modelling using centroid = 2
set.seed(5)
model_k <- kmeans(data.mod,2, iter.max = 100)
#Viewing centers of the clusters
model_k$centers

```
```{r}
model_k$cluster

```
```{r}

# Previewing the no. of records in each cluster
# 
model_k$size 
```


```{r}
t1 <- table(model_k$cluster, mod.class)
t1

```

```{r}
fviz_cluster(model_k, data = data.mod,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw()
             )
```

**Conclusion**
From the K Means Clustering, using two centroids and a 100 iterations, we can see that the algorithm did a good job by classifying most True's in the revenue column as true but did not cluster the false group well. It clusters well to around 50% on both 

## Challenging the solution
Since our K means algorithm does not classify our data very well when compared to the classification in the original dataset we seek to challenge it by using the **Hierarchical Clustering**

### Hierarchical Clustering
```{r}
library(cluster)
library(dendextend)
library(purrr)
```



```{r}
# Dissimilarity matrix
#mod.new <- as.dist(mod.new)
#hclust(object_name, method = "single")
d <- dist(mod.new, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)

```

```{r}
# Hierarchical clustering using Average Linkage
hc2 <- hclust(d, method = "average" )

# Plot the obtained dendrogram
plot(hc2, cex = 0.6, hang = -1)
```
```{r}
# Hierarchical clustering using Average Linkage
hc3 <- hclust(d, method = "ward" )

# Plot the obtained dendrogram
plot(hc3, cex = 0.6, hang = -1)

```
**Conclusions**
The Complete method for hierachical clustering seems to classify better than the rest. It would therefore be preferred for this segmentation as compared to the rest






